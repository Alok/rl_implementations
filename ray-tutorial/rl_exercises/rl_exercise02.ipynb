{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 2 - Derivative Free Optimization\n",
    "\n",
    "**GOAL:** The goal if this exercise is to show how to use Ray to implement a simple Monte Carlo algorithm for reinforcement learning.\n",
    "\n",
    "The goal of reinforcement learning is to find a policy (parameterized by $\\pi$), which solves the following optimization problem.\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=0}^T R_t(\\pi)\n",
    "\\end{equation}\n",
    "\n",
    "Here, $R_t$ is the reward received at time $t$ when acting according to the policy $\\pi$. Note that if the environment is stochastic or the policy is stochastic, then each $R_t$ will be a random variable. Also note that $T$ will be a random variable. Both $R_t$ and $T$ depend on $\\pi$.\n",
    "\n",
    "Though the setup is similar to supervised learning in that in both settings we want to minimize or maximize some objective function, in supervised learning we often have an explicit formula for the objective function in terms of the parameters of interest, which enables us to symbolically compute the gradient of the objective function. So in supervised learning, we can often directly apply gradient descent to optimize the objective.\n",
    "\n",
    "In reinforcement learning, we often do not have an explicit formula for the reward function that we are trying to optimize, and so we can't easily compute gradients. For example, imagine an environment in which a robot walks until it falls over and the reward is the distance that the robot walked before it fell over. Computing the gradient of that reward with respect to the parameters of the robot's policy is not straightforward.\n",
    "\n",
    "The difficulty of computing explicit gradients motivates the use of **derivative free optimization**. We will work through some examples below.\n",
    "\n",
    "**NOTE:** There is a huge variety of much more sophisticated RL algorithms. Here we are walking through the details of implementing a simple Monte Carlo algorithm. Subsequent exercises will show how to apply more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:32:52.275322Z",
     "start_time": "2018-03-29T03:32:51.488422Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:32:53.497457Z",
     "start_time": "2018-03-29T03:32:52.402679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for redis server at 127.0.0.1:32939 to respond...\n",
      "Waiting for redis server at 127.0.0.1:24303 to respond...\n",
      "Starting local scheduler with the following resources: {'CPU': 8, 'GPU': 0}.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8893/notebooks/ray_ui83512.ipynb?token=d03236d39b43583ee988b3600226ff4b97b47687463548aa\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'local_scheduler_socket_names': ['/tmp/scheduler29728445'],\n",
       " 'node_ip_address': '127.0.0.1',\n",
       " 'object_store_addresses': [ObjectStoreAddress(name='/tmp/plasma_store77611340', manager_name='/tmp/plasma_manager83476043', manager_port=16384)],\n",
       " 'redis_address': '127.0.0.1:32939',\n",
       " 'webui_url': 'http://localhost:8893/notebooks/ray_ui83512.ipynb?token=d03236d39b43583ee988b3600226ff4b97b47687463548aa'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a policy that chooses an action using a randomly-generated two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:33:08.859135Z",
     "start_time": "2018-03-29T03:33:08.805047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerPolicy(object):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs=1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_units = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights1 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases1 = np.random.normal(size=num_hiddens)\n",
    "        self.weights2 = np.random.normal(size=(num_outputs, num_hiddens))\n",
    "        self.biases2 = np.random.normal(size=num_outputs)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        hiddens = np.maximum(np.dot(self.weights1, state) + self.biases1, 0)\n",
    "        output = np.dot(self.weights2, hiddens) + self.biases2\n",
    "        assert output.size == 1\n",
    "        return 0 if output[0] < 0 else 1\n",
    "\n",
    "policy = TwoLayerPolicy(4, 5)\n",
    "# You can get an action by applying the policy to a state.\n",
    "action = policy(np.random.normal(size=4))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remote function `evaluate_random_policy` defined below generates a random `TwoLayerPolicy`, performs some rollouts using a CartPole environment, and returns the average reward over those rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:33:36.807126Z",
     "start_time": "2018-03-29T03:33:36.670468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.6\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You may find the helper function 'rollout_policy' helpful.\n",
    "# This implementation here is the solution to one of the exercises\n",
    "# from the previous notebook.\n",
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose an action.\n",
    "        action = policy(state)\n",
    "        # Take an action.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "@ray.remote\n",
    "def evaluate_random_policy(num_rollouts):\n",
    "    # Generate a random policy.\n",
    "    policy = TwoLayerPolicy(4, 5)\n",
    "    \n",
    "    # Create an environment.\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    # We evaluate the same policy multiple times and then take the average\n",
    "    # in order to evaluate the policy more accurately (the environment is\n",
    "    # stochastic).\n",
    "    return np.mean([rollout_policy(env, policy) for _ in range(num_rollouts)])\n",
    "    \n",
    "average_reward = ray.get(evaluate_random_policy.remote(10))\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Using the `evaluate_random_policy` remote function, evaluate 100 randomly generated policies. Make a note of the best score. Try taking the best of 1000.\n",
    "\n",
    "**NOTE:** The best possible score should be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:43:35.916440Z",
     "start_time": "2018-03-29T03:43:35.001474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.71\n"
     ]
    }
   ],
   "source": [
    "# Evaluate 100 randomly generated policies.\n",
    "scores = ray.get([evaluate_random_policy.remote(100) for _ in range(100)])\n",
    "\n",
    "# Print the best score obtained.\n",
    "print(max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
